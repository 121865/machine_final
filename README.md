# machine_final$``$
## Ping Pong
- ### éœ€æ±‚
  - åŠŸèƒ½
    ä¸Šã€ä¸‹ã€åœæ­¢ä¸‰ç¨®
  - æ•ˆèƒ½
  - é™åˆ¶
  - é©—æ”¶
- ### é è¨ˆä½¿ç”¨æ¼”ç®—æ³•
#### Proximal Policy Optimization (PPO)
---
å¼·åŒ–å­¸ç¿’ä¸‰å€‹Components:Agentã€Enviromentã€Reward Function  
On-policy:è·Ÿç’°å¢ƒäº’å‹•çš„Agentèˆ‡è¨“ç·´çš„Agentç‚ºåŒä¸€å€‹  
Off-policy:è·Ÿç’°å¢ƒäº’å‹•çš„Agentèˆ‡è¨“ç·´çš„AgentéåŒä¸€å€‹  

__On-policy Gradient:__
```math
E_{(s_t, a_t) \sim \pi_\theta} \left[ A^\theta(s_t, a_t) \nabla \log p_\theta(a^n_t \vert s^n_t) \right]
```

  * $`(a^n_t \vert s^n_t)`$æ˜¯æ•´å€‹trajectoryå…§çš„æŸä¸€å€‹æ™‚é–“é»çš„æˆå°è³‡æ–™(pair)
    * å¦‚æœé€™å€‹pairæœƒå°è‡´æ•´å€‹trajectoryçš„rewardè®Šå¤§ï¼Œé‚£å°±è¦å¢åŠ å®ƒå‡ºç¾çš„æ©Ÿç‡ï¼Œåä¹‹å‰‡æ¸›å°‘ã€‚
  * $`A^\theta(s_t,a_t)`$:åœ¨æŸä¸€å€‹state-$`s_t`$åŸ·è¡ŒæŸä¸€å€‹action-$`a_t`$ï¼Œç›¸è¼ƒæ–¼å…¶å®ƒå¯èƒ½çš„actionï¼Œç¾åœ¨åŸ·è¡Œçš„é€™ä¸€å€‹æœ‰å¤šå¥½ã€‚

<img width="681" height="233" alt="image" src="https://github.com/user-attachments/assets/8b8b1d4d-56b5-42fd-94ee-c702c2305d2e" />

---
__Importance Sampling__:
```math
  E_{x \sim p}\left[f(x) \right]=E_{x \sim q} \left[f(x) \dfrac{p(x)}{q(x)} \right]
```  
* __æ¨å°å¦‚ä¸‹:__  
  * $`E_{x \sim p}\left[f(x) \right]\approx \dfrac{1}{N} \sum^N_{i=1} f(x^i)`$  
    * æ²’æœ‰è¾¦æ³•å°distribution-ğ‘è¨ˆç®—ç©åˆ†ï¼Œå¯ä»¥ç”¨sampleçš„æ–¹å¼ï¼Œå–å¹³å‡å€¼ä¾†è¿‘ä¼¼æœŸæœ›å€¼  
  * $`=\int f(x) p(x) dx`$
    * å°$`p(x)`$è¨ˆç®—ç©åˆ†
  * $`=\int f(x) \dfrac{p(x)}{q(x)} q(x) dx`$
    * åˆ†å­åˆ†æ¯åŒä¹˜$`q(x)`$
  * $`=E_{x \sim q} \left[f(x) \dfrac{p(x)}{q(x)} \right]`$
    * èª¿æ•´ç‚ºå¾$`q`$ä¾†sampleå‡º$`x`$å–æœŸæœ›å€¼
    * éœ€è¦ä¹˜ä¸Šä¸€å€‹æ¬Šé‡$`\dfrac{p(x)}{q(x)}`$ä¾†ä¿®æ­£$`p`$,$`q`$å…©å€‹distributionä¹‹é–“çš„å·®ç•°



__Off-policy Gradient:__
```math
E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \dfrac{P_\theta(s_t, a_t)}{P_{\theta'}(s_t, a_t)} A^{\theta'}(s_t, a_t) \nabla \log p_\theta(a^n_t \vert s^n_t) \right]
```
æ©Ÿç‡æ‹†è§£å¦‚ä¸‹  
```math
E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \dfrac{P_\theta(a_t \vert s_t)}{P_{\theta'}(a_t \vert s_t)} \dfrac{p_\theta(s_t)}{p_{\theta'}(s_t)} A^{\theta'}(s_t, a_t) \nabla \log p_\theta(a^n_t \vert s^n_t) \right]
```
  * å‡è¨­æ¨¡å‹åœ¨$`\theta`$èˆ‡$`\theta'`$çœ‹åˆ°$`s_t`$çš„æ©Ÿç‡æ˜¯å·®ä¸å¤šçš„ï¼Œå› æ­¤åˆªé™¤ã€‚
  * å¦ä¸€å€‹æƒ³æ³•ï¼Œ$`s_t`$é›£ä»¥ä¼°æ¸¬ï¼Œå› æ­¤ç„¡è¦–ã€‚


å¯ä»¥è—‰ç”±æ­¤å…¬å¼<img width="204" height="36" alt="image" src="https://github.com/user-attachments/assets/aa7e6fc1-3846-4b61-8390-2edcc4c78536" />
åæ¨å¾—**Objective Function**  
```math
  J^{\theta'}(\theta) = E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \dfrac{P_\theta(a_t \vert s_t)}{P_{\theta'}(a_t \vert s_t)} A^{\theta'}(s_t, a_t) \right]
```

ç‚ºäº†é¿å…$`\theta`$èˆ‡$`\theta'`$å·®å¤ªå¤šéœ€è¦åŠ å€‹constraint  
<img width="309" height="56" alt="image" src="https://github.com/user-attachments/assets/72c664d0-3ec4-4c18-b552-16774d41659e" />  
<img width="80" height="31" alt="image" src="https://github.com/user-attachments/assets/c9e464bc-1f3f-4665-a2ec-6826b97e4e56" />æ•£åº¦æ˜¯ç‚ºäº†åˆ¤å®šå…©è€…çš„behavioræˆ–è€…æ˜¯actionæœ‰å¤šåƒ;$`\beta`$è¨­å®š  
<img width="528" height="123" alt="image" src="https://github.com/user-attachments/assets/3004c96c-e181-4c94-877d-9c0e2191152d" />  





- ### Loss function
```math
L(\theta) = - J^{\theta'}(\theta)
```
```math
L(\theta) = - E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \dfrac{P_\theta(a_t \vert s_t)}{P_{\theta'}(a_t \vert s_t)} A^{\theta'}(s_t, a_t) \right]
```
## Chess
* ### é è¨ˆä½¿ç”¨æ¼”ç®—æ³•
<mark>Soft Actor-Critic (SAC) <mark>  
  
**ç°¡ä»‹ :**
å‰èº«ç‚ºSoft Q-learningï¼Œå› ç‚ºSoft Q-learning æ˜¯ä¸€å€‹ä½¿ç”¨å‡½æ•¸Qçš„Boltzman distributionï¼Œåœ¨é€£çºŒç©ºé–“ä¸‹æ±‚è§£éº»ç…©ï¼Œæ‰€ä»¥æå‡ºäº†**Actor**è¡¨ç¤ºç­–ç•¥å‡½æ•¸(Policy Function)ï¼Œå±¬æ–¼Off-policyã€‚  
  
* ### SACçš„Object Function  
  
```math
J(\pi) = ğ”¼ _\pi \left[ \sum \limits _{t=0} ^{\infty} \gamma ^t (r(s_t,a_t) + \alpha H  (\pi(\cdot|s_t)))\right]
```
å®šç¾© :  
$`J(\pi)`$ : æ•´å€‹SACæƒ³æœ€å¤§åŒ–çš„ç›®æ¨™å‡½æ•¸ï¼Œä»£è¡¨ç­–ç•¥ $`\pi`$ çš„å¥½å£ã€‚  
$`ğ”¼_\pi [\cdot]`$ : æœŸæœ›å€¼ï¼Œä»£è¡¨ã€Œç…§è‘—ç­–ç•¥ $`\pi`$ èˆ‡ç’°å¢ƒäº’å‹•ã€æ‰€å¾—åˆ°çš„å¹³å‡çµæœã€‚  
$`\sum \limits _{t=0} ^\infty`$ : æŠŠæ•´å€‹éç¨‹æ‰€æœ‰æ™‚é–“æ­¥çš„å›å ±ç´¯åŠ ã€‚  
$`\gamma^t`$ : æŠ˜æ‰£å› å­(discoun factor) ï¼Œä»‹æ–¼0~1ä¹‹é–“ï¼Œè¶Šä¹…é çš„å›å ±æ¬Šé‡è¶Šä½ã€‚  
$`r(s_t,a_t)`$ : reward functionï¼Œåœ¨ç‹€æ…‹ $`s_t`$ åšå‹•ä½œ $`a_t`$ å¾—åˆ°çš„ç«‹å³å›é¥‹ã€‚  
$`\alpha H (\pi(\cdot|s_t))`$ : æ¢ç´¢çå‹µ(entropy bouns)ï¼Œç”± $`\alpha`$ è·Ÿ $`H (\pi(\cdot|s_t))`$ çµ„æˆï¼Œã€Œè¡Œç‚ºè¶Šå¤šæ¨£åŒ– $`\to`$ entropyè¶Šé«˜ $\to$ æ¢ç´¢è¶Šå¤šã€ã€‚  
$`\alpha`$ : æº«åº¦ä¿‚æ•¸(temperature/entropy weight) ï¼Œæ§åˆ¶entropyçš„é‡è¦ç¨‹åº¦ã€‚ $`\alpha`$ è¶Šå¤§ $\to$ è¶Šé¼“å‹µæ¢ç´¢ï¼›è¶Šå° $`\to`$ è¶Šé¼“å‹µåˆ©ç”¨ã€‚  
$`H (\pi(\cdot|s_t))`$ : policyåœ¨ state $`s_t`$ çš„entropy ï¼Œè¨ˆç®—å…¬å¼ç­‰æ–¼ $`-ğ”¼_{a\sim \pi(\cdot|s_t)}\left[log\pi(a_t|s_t) \right]`$ã€‚   
* ### Critic Loss (Q-networkçš„loss)
  
```math
L_Q(\omega) = ğ”¼_{(s_t,a_t,r_t,s_{t+1})\sim R}\left[{1\over 2}(Q_\omega (s_t,a_t) - y_t)^2  \right]  
```
å®šç¾© :  
$`L_Q(\omega)`$ : Qç¶²è·¯è¦æœ€å°åŒ–çš„æå¤±ï¼Œè€Œ $`\omega`$ æ˜¯Qç¶²è·¯çš„åƒæ•¸(weights)ã€‚  
$`ğ”¼_{(s_t,a_t,r_t,s_{t+1})\sim R}[\cdot]`$ : å¾ç¶“é©—å›æ”¾ç·©è¡å€(Replay Buffer)ä¸­éš¨æ©Ÿå–æ¨£ä¸€å€‹transitionåšæœŸæœ›ï¼Œä¹Ÿå°±æ˜¯Qæ˜¯ç”¨off-policyè³‡æ–™è¨“ç·´ã€‚R $`\to`$ ç¶“é©—æ•¸æ“šçš„åˆ†å¸ƒæˆ–é›†åˆã€‚  
$`{1\over 2}(\cdot)^2`$ : å‡æ–¹èª¤å·®(MSE)ï¼Œå¸Œæœ›Qçš„è¼¸å‡ºè¶Šæ¥è¿‘ç›®æ¨™ $`y_t`$ã€‚  
$`Q_\omega (s_t,a_t)`$ : åœ¨ç‹€æ…‹ $`s_t`$ åŸ·è¡Œå‹•ä½œ $`a_t`$ çš„é æœŸç¸½å›å ±(å« entropy)ï¼Œä¹Ÿå°±æ˜¯Q-networkè¼¸å‡ºé€™å€‹state-actionçš„åƒ¹å€¼ã€‚  
$`y_t`$ : å¯¦éš›æ‡‰è©²è¦æ¥è¿‘çš„åƒ¹å€¼(Target value) ï¼Œç­‰æ–¼ $`r_t + \gamma(\min \limits_j Q_{\bar{\omega}_j}(s_{t+1},a_{t+1}) - \alpha log \pi (a_{t+1}|s_{t+1}) )`$    
$`r_t`$ : ç•¶ä¸‹reward ã€‚  
$`\gamma`$ : æŠ˜æ‰£å› å­ã€‚  
$`\min\limits_j Q_{\bar{\omega}_j}(s_{t+1},a_{t+1})`$ : ç”¨å…©å€‹target Q-netçš„æœ€å°å€¼ï¼Œé¿å…é«˜ä¼°(Double Qçš„æŠ€å·§)ã€‚  
$`\bar{\omega}`$ : target Q-networkçš„åƒæ•¸(æ…¢æ…¢æ›´æ–°çš„Qï¼Œç”¨ä¾†ç©©å®šè¨“ç·´)ã€‚  
$`\alpha log \pi (a_{t+1}|s_{t+1})`$ : ä¸‹ä¸€æ­¥entropy bounsã€‚  
  
* ### Policy Loss (actorçš„Loss)
  
```math
L_\pi (\theta) = ğ”¼_{s_t \sim R,a_t \sim \pi_\theta} \left[\alpha log \pi_\theta (a_t|s_t) - Q_\omega (s_t,a_t) \right]
```
    
å®šç¾© :  
$`L_\pi (\theta)`$ : actor(policy network)è¦æœ€å°åŒ–çš„æå¤±ï¼Œ $`\theta`$ ç‚º policy network çš„åƒæ•¸ã€‚  
$`s_t \sim R`$ : ç‹€æ…‹å¾replay bufferæŠ½æ¨£(off-policy) ã€‚  
$`a_t \sim \pi_\theta(\cdot|s_t)`$ : åœ¨ state $`s_t`$ä¸‹ï¼Œå¾policy $`\pi`$ å–æ¨£å‹•ä½œã€‚  
$`\alpha log \pi_\theta (a_t|s_t)`$ : è¶Šç¢ºå®šçš„å‹•ä½œæ©Ÿç‡è¶Šæ¥è¿‘1 $`\to`$ log $`\pi`$ è¶Šå¤§(è² çš„)ï¼Œä¹Ÿå¯èªªæ˜¯é€™é …çš„æ•ˆæœç‚º **å¢åŠ entropyä¸”é¼“å‹µè¡Œç‚ºæ›´éš¨æ©Ÿ** ã€‚  
$`-Q_\omega (s_t,a_t)`$ : è‹¥Qå€¼è¶Šå¤§å‰‡é€™é …è² æ•¸å¤§ $`\to`$ æœ‰åŠ©æ–¼é™ä½lossï¼Œé¼“å‹µé¸æ“‡Qé«˜çš„è¡Œç‚ºã€‚  
  
* ###  $\alpha$  Loss
  
```math
L(\alpha) = ğ”¼_{a_t \sim \pi} \left[- \alpha log \pi (a_t|s_t) - \alpha H_0 \right]
```
**ç›®çš„ç‚ºè‡ªå‹•èª¿æ•´ $`\alpha`$ ä½¿ : $`ğ”¼[-log\pi] = H_0`$ï¼Œentropyè‡ªå‹•ç¶­æŒåœ¨å¸Œæœ›çš„æ°´æº–ã€‚**  
  
å®šç¾© :  
$`L(\alpha)`$ : å°ˆé–€ç”¨ä¾†æ›´æ–° $`\alpha`$ çš„ lossã€‚  
$`- \alpha log \pi (a_t|s_t)`$ : ç•¶ç­–ç•¥éæ–¼ç¢ºå®š(entropyå¤ªä½)æ™‚ï¼Œ $`log\pi`$æœƒè®Šå°ï¼Œlossåå¤§æœƒæ¨å‹• $`\alpha`$ æé«˜ $\to$ ä¿ƒä½¿ç­–ç•¥æ›´éš¨æ©Ÿã€‚  
$`-\alpha H_0`$ : $H_0$ æ˜¯ç›®æ¨™entropyï¼Œè®“ç­–ç•¥çš„entropyæœå›ºå®šç›®æ¨™é è¿‘ã€‚  
  
* ### Reparameterization Function(SAC core) 
  
```math
a_t = f_\theta(\epsilon_t ;s_t) ï¼Œ \epsilon_t \sim N(0,I)
```

**è®“policyæŠ½æ¨£è®Šæˆå¯å¾® $\to$ å¯ä»¥ç”¨backpropè¨“ç·´actorã€‚**  
  
å®šç¾© :  
$`f_\theta`$ : ä¸€å€‹å¯å¾®åˆ†å‡½æ•¸ï¼Œé€šå¸¸æ˜¯ $`f_\theta(\epsilon,s) = tanh(\mu_\theta(s_t) + \sigma _\theta(s_t) \cdot \epsilon_t)`$ ï¼ŒåŒ…å«é«˜æ–¯åˆ†å¸ƒå–æ¨£( $`u_t = \mu_\theta(s_t) + \sigma_\theta(s_t) \cdot \epsilon_t`$ ) è·Ÿ tanhç¸®æ”¾( $`a_t = tanh(u_t)`$ )  
$`\epsilon_t \sim N(0,I)`$ : å¾æ¨™æº–å¸¸æ…‹N(0,1) å–çš„noiseï¼Œæä¾›éš¨æ©Ÿæ€§ã€‚  
  
* ### Soft Value Function
  
```math
V(s_t) = ğ”¼_{a_t \sim \pi} \left[Q(s_t,a_t) - \alpha log \pi(a_t|s_t) \right]
```
  
**$`V(s_t)`$ = å¹³å‡ã€Œé¸åˆ°çš„Qå€¼ + è©²å‹•ä½œçš„æ¢ç´¢çå‹µã€ã€‚**  
å®šç¾© :  
$`V(s_t)`$ : åœ¨ç‹€æ…‹ $`s_t`$ çš„é æœŸç¸½åƒ¹å€¼ï¼Œä½†soft valueä¸åªæ˜¯rewardï¼Œä¹ŸåŒ…å« entropy bounsã€‚  
$`ğ”¼_{a_t \sim \pi}[\cdot]`$ : ç”±ç­–ç•¥ $`\pi`$ å–æ¨£å‹•ä½œã€‚  
$`Q(s_t,a_t)`$ : è©²å‹•ä½œçš„Q-value(å›å ±ç¸½æœŸæœ›)ã€‚  
$`-\alpha log \pi(a_t|s_t)`$ : ä»£è¡¨æ¢ç´¢bounsï¼Œè¶Šéš¨æ©Ÿè¶Šæœ‰çå‹µã€‚  

- ### æ‡‰ç”¨  
ç‹€æ…‹åƒ¹å€¼å‡½æ•¸ : è¡¡é‡ç•¶å‰å±€é¢çš„å¥½å£  
ç­–ç•¥å‡½æ•¸ : æ±ºå®šæ¨¡å‹åœ¨æ£‹ç›¤ä¸Šçš„èµ°æ³•é¸æ“‡å‚¾å‘  
loss function : LQâ€‹(Ï‰) --> è©•åƒ¹æ¯æ­¥æ£‹çš„å¥½å£  
                LÏ€â€‹(Î¸) --> è¼¸å‡ºæ¯ä¸€æ­¥æ£‹çš„æ©Ÿç‡åˆ†ä½ˆï¼Œè‹¥å¤šæ­¥æ£‹ Q å€¼æ¥è¿‘entropy æœƒé¼“å‹µæ¨¡å‹ç¹¼çºŒæ¢ç´¢å…¶ä»–å¯è¡Œèµ°æ³•  
                L(Î±)  --> æ¨¡å‹å¤ªä¿å®ˆã€è€æ˜¯èµ°åŒä¸€å¥—é–‹å±€å‰‡æé«˜ Î±ï¼Œå¼·è¿«å˜—è©¦æ–°ç­–ç•¥ ï¼›æ¨¡å‹å¤ªäº‚ã€åƒäº‚ä¸‹æ£‹å‰‡é™ä½ Î±ï¼Œä½¿æ±ºç­–æ›´ç©©å®š  
- ### Reference
<https://hackmd.io/@shaoeChen/Bywb8YLKS/https%3A%2F%2Fhackmd.io%2F%40shaoeChen%2FSyez2AmFr#PPO-algorithm>  
<https://hrl.boyuai.com/chapter/2/sac%E7%AE%97%E6%B3%95/>


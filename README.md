# machine_final
## åˆ†å·¥
çµ„é•· : F114112116 åŠ‰æ­£å‰ è² è²¬ä¹’ä¹“çƒ  
çµ„å“¡ : F114112114 èƒ¡å®¶è±ª è² è²¬è¥¿æ´‹æ£‹  

## Ping Pong
- ### æ–‡ç»å›é¡§
#### Proximal Policy Optimization (PPO)
---
å¼·åŒ–å­¸ç¿’ä¸‰å€‹Components:Agentã€Enviromentã€Reward Function  
On-policy:è·Ÿç’°å¢ƒäº’å‹•çš„Agentèˆ‡è¨“ç·´çš„Agentç‚ºåŒä¸€å€‹  
Off-policy:è·Ÿç’°å¢ƒäº’å‹•çš„Agentèˆ‡è¨“ç·´çš„AgentéåŒä¸€å€‹  

__On-policy Gradient:__
```math
E_{(s_t, a_t) \sim \pi_\theta} \left[ A^\theta(s_t, a_t) \nabla \log p_\theta(a^n_t \vert s^n_t) \right]
```

  * $`(a^n_t \vert s^n_t)`$æ˜¯æ•´å€‹trajectoryå…§çš„æŸä¸€å€‹æ™‚é–“é»çš„æˆå°è³‡æ–™(pair)
    * å¦‚æœé€™å€‹pairæœƒå°è‡´æ•´å€‹trajectoryçš„rewardè®Šå¤§ï¼Œé‚£å°±è¦å¢åŠ å®ƒå‡ºç¾çš„æ©Ÿç‡ï¼Œåä¹‹å‰‡æ¸›å°‘ã€‚
  * $`A^\theta(s_t,a_t)`$:åœ¨æŸä¸€å€‹state-$`s_t`$åŸ·è¡ŒæŸä¸€å€‹action-$`a_t`$ï¼Œç›¸è¼ƒæ–¼å…¶å®ƒå¯èƒ½çš„actionï¼Œç¾åœ¨åŸ·è¡Œçš„é€™ä¸€å€‹æœ‰å¤šå¥½ã€‚

<img width="681" height="233" alt="image" src="https://github.com/user-attachments/assets/8b8b1d4d-56b5-42fd-94ee-c702c2305d2e" />

---
__Importance Sampling__:
```math
  E_{x \sim p}\left[f(x) \right]=E_{x \sim q} \left[f(x) \dfrac{p(x)}{q(x)} \right]
```  
* __æ¨å°å¦‚ä¸‹:__  
  * $`E_{x \sim p}\left[f(x) \right]\approx \dfrac{1}{N} \sum^N_{i=1} f(x^i)`$  
    * æ²’æœ‰è¾¦æ³•å°distribution-ğ‘è¨ˆç®—ç©åˆ†ï¼Œå¯ä»¥ç”¨sampleçš„æ–¹å¼ï¼Œå–å¹³å‡å€¼ä¾†è¿‘ä¼¼æœŸæœ›å€¼  
  * $`=\int f(x) p(x) dx`$
    * å°$`p(x)`$è¨ˆç®—ç©åˆ†
  * $`=\int f(x) \dfrac{p(x)}{q(x)} q(x) dx`$
    * åˆ†å­åˆ†æ¯åŒä¹˜$`q(x)`$
  * $`=E_{x \sim q} \left[f(x) \dfrac{p(x)}{q(x)} \right]`$
    * èª¿æ•´ç‚ºå¾$`q`$ä¾†sampleå‡º$`x`$å–æœŸæœ›å€¼
    * éœ€è¦ä¹˜ä¸Šä¸€å€‹æ¬Šé‡$`\dfrac{p(x)}{q(x)}`$ä¾†ä¿®æ­£$`p`$,$`q`$å…©å€‹distributionä¹‹é–“çš„å·®ç•°



__Off-policy Gradient:__
```math
E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \dfrac{P_\theta(s_t, a_t)}{P_{\theta'}(s_t, a_t)} A^{\theta'}(s_t, a_t) \nabla \log p_\theta(a^n_t \vert s^n_t) \right]
```
æ©Ÿç‡æ‹†è§£å¦‚ä¸‹  
```math
E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \dfrac{P_\theta(a_t \vert s_t)}{P_{\theta'}(a_t \vert s_t)} \dfrac{p_\theta(s_t)}{p_{\theta'}(s_t)} A^{\theta'}(s_t, a_t) \nabla \log p_\theta(a^n_t \vert s^n_t) \right]
```
  * å‡è¨­æ¨¡å‹åœ¨$`\theta`$èˆ‡$`\theta'`$çœ‹åˆ°$`s_t`$çš„æ©Ÿç‡æ˜¯å·®ä¸å¤šçš„ï¼Œå› æ­¤åˆªé™¤ã€‚
  * å¦ä¸€å€‹æƒ³æ³•ï¼Œ$`s_t`$é›£ä»¥ä¼°æ¸¬ï¼Œå› æ­¤ç„¡è¦–ã€‚


å¯ä»¥è—‰ç”±æ­¤å…¬å¼<img width="204" height="36" alt="image" src="https://github.com/user-attachments/assets/aa7e6fc1-3846-4b61-8390-2edcc4c78536" />
åæ¨å¾—**Objective Function**  
```math
  J^{\theta'}(\theta) = E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \dfrac{P_\theta(a_t \vert s_t)}{P_{\theta'}(a_t \vert s_t)} A^{\theta'}(s_t, a_t) \right]
```

ç‚ºäº†é¿å…$`\theta`$èˆ‡$`\theta'`$å·®å¤ªå¤šéœ€è¦åŠ å€‹constraint  
<img width="309" height="56" alt="image" src="https://github.com/user-attachments/assets/72c664d0-3ec4-4c18-b552-16774d41659e" />  
<img width="80" height="31" alt="image" src="https://github.com/user-attachments/assets/c9e464bc-1f3f-4665-a2ec-6826b97e4e56" />æ•£åº¦æ˜¯ç‚ºäº†åˆ¤å®šå…©è€…çš„behavioræˆ–è€…æ˜¯actionæœ‰å¤šåƒ;$`\beta`$è¨­å®š  
<img width="528" height="123" alt="image" src="https://github.com/user-attachments/assets/3004c96c-e181-4c94-877d-9c0e2191152d" />  





- ### Loss function
```math
L(\theta) = - J^{\theta'}(\theta)
```
```math
L(\theta) = - E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \dfrac{P_\theta(a_t \vert s_t)}{P_{\theta'}(a_t \vert s_t)} A^{\theta'}(s_t, a_t) \right]
```
---







  




## Chess
* ### é è¨ˆä½¿ç”¨æ¼”ç®—æ³•
<mark>Soft Actor-Critic (SAC) <mark>  
  
**ç°¡ä»‹ :**
å‰èº«ç‚ºSoft Q-learningï¼Œå› ç‚ºSoft Q-learning æ˜¯ä¸€å€‹ä½¿ç”¨å‡½æ•¸Qçš„Boltzman distributionï¼Œåœ¨é€£çºŒç©ºé–“ä¸‹æ±‚è§£éº»ç…©ï¼Œæ‰€ä»¥æå‡ºäº†**Actor**è¡¨ç¤ºç­–ç•¥å‡½æ•¸(Policy Function)ï¼Œå±¬æ–¼Off-policyã€‚  
  
* ### SACçš„Object Function  
  
```math
J(\pi) = ğ”¼ _\pi \left[ \sum \limits _{t=0} ^{\infty} \gamma ^t (r(s_t,a_t) + \alpha H  (\pi(\cdot|s_t)))\right]
```
å®šç¾© :  
$`J(\pi)`$ : æ•´å€‹SACæƒ³æœ€å¤§åŒ–çš„ç›®æ¨™å‡½æ•¸ï¼Œä»£è¡¨ç­–ç•¥ $`\pi`$ çš„å¥½å£ã€‚  
$`ğ”¼_\pi [\cdot]`$ : æœŸæœ›å€¼ï¼Œä»£è¡¨ã€Œç…§è‘—ç­–ç•¥ $`\pi`$ èˆ‡ç’°å¢ƒäº’å‹•ã€æ‰€å¾—åˆ°çš„å¹³å‡çµæœã€‚  
$`\sum \limits _{t=0} ^\infty`$ : æŠŠæ•´å€‹éç¨‹æ‰€æœ‰æ™‚é–“æ­¥çš„å›å ±ç´¯åŠ ã€‚  
$`\gamma^t`$ : æŠ˜æ‰£å› å­(discoun factor) ï¼Œä»‹æ–¼0~1ä¹‹é–“ï¼Œè¶Šä¹…é çš„å›å ±æ¬Šé‡è¶Šä½ã€‚  
$`r(s_t,a_t)`$ : reward functionï¼Œåœ¨ç‹€æ…‹ $`s_t`$ åšå‹•ä½œ $`a_t`$ å¾—åˆ°çš„ç«‹å³å›é¥‹ã€‚  
$`\alpha H (\pi(\cdot|s_t))`$ : æ¢ç´¢çå‹µ(entropy bouns)ï¼Œç”± $`\alpha`$ è·Ÿ $`H (\pi(\cdot|s_t))`$ çµ„æˆï¼Œã€Œè¡Œç‚ºè¶Šå¤šæ¨£åŒ– $`\to`$ entropyè¶Šé«˜ $\to$ æ¢ç´¢è¶Šå¤šã€ã€‚  
$`\alpha`$ : æº«åº¦ä¿‚æ•¸(temperature/entropy weight) ï¼Œæ§åˆ¶entropyçš„é‡è¦ç¨‹åº¦ã€‚ $`\alpha`$ è¶Šå¤§ $\to$ è¶Šé¼“å‹µæ¢ç´¢ï¼›è¶Šå° $`\to`$ è¶Šé¼“å‹µåˆ©ç”¨ã€‚  
$`H (\pi(\cdot|s_t))`$ : policyåœ¨ state $`s_t`$ çš„entropy ï¼Œè¨ˆç®—å…¬å¼ç­‰æ–¼ $`-ğ”¼_{a\sim \pi(\cdot|s_t)}\left[log\pi(a_t|s_t) \right]`$ã€‚   
* ### Critic Loss (Q-networkçš„loss)
  
```math
L_Q(\omega) = ğ”¼_{(s_t,a_t,r_t,s_{t+1})\sim R}\left[{1\over 2}(Q_\omega (s_t,a_t) - y_t)^2  \right]  
```
å®šç¾© :  
$`L_Q(\omega)`$ : Qç¶²è·¯è¦æœ€å°åŒ–çš„æå¤±ï¼Œè€Œ $`\omega`$ æ˜¯Qç¶²è·¯çš„åƒæ•¸(weights)ã€‚  
$`ğ”¼_{(s_t,a_t,r_t,s_{t+1})\sim R}[\cdot]`$ : å¾ç¶“é©—å›æ”¾ç·©è¡å€(Replay Buffer)ä¸­éš¨æ©Ÿå–æ¨£ä¸€å€‹transitionåšæœŸæœ›ï¼Œä¹Ÿå°±æ˜¯Qæ˜¯ç”¨off-policyè³‡æ–™è¨“ç·´ã€‚R $`\to`$ ç¶“é©—æ•¸æ“šçš„åˆ†å¸ƒæˆ–é›†åˆã€‚  
$`{1\over 2}(\cdot)^2`$ : å‡æ–¹èª¤å·®(MSE)ï¼Œå¸Œæœ›Qçš„è¼¸å‡ºè¶Šæ¥è¿‘ç›®æ¨™ $`y_t`$ã€‚  
$`Q_\omega (s_t,a_t)`$ : åœ¨ç‹€æ…‹ $`s_t`$ åŸ·è¡Œå‹•ä½œ $`a_t`$ çš„é æœŸç¸½å›å ±(å« entropy)ï¼Œä¹Ÿå°±æ˜¯Q-networkè¼¸å‡ºé€™å€‹state-actionçš„åƒ¹å€¼ã€‚  
$`y_t`$ : å¯¦éš›æ‡‰è©²è¦æ¥è¿‘çš„åƒ¹å€¼(Target value) ï¼Œç­‰æ–¼ $`r_t + \gamma(\min \limits_j Q_{\bar{\omega}_j}(s_{t+1},a_{t+1}) - \alpha log \pi (a_{t+1}|s_{t+1}) )`$    
$`r_t`$ : ç•¶ä¸‹reward ã€‚  
$`\gamma`$ : æŠ˜æ‰£å› å­ã€‚  
$`\min\limits_j Q_{\bar{\omega}_j}(s_{t+1},a_{t+1})`$ : ç”¨å…©å€‹target Q-netçš„æœ€å°å€¼ï¼Œé¿å…é«˜ä¼°(Double Qçš„æŠ€å·§)ã€‚  
$`\bar{\omega}`$ : target Q-networkçš„åƒæ•¸(æ…¢æ…¢æ›´æ–°çš„Qï¼Œç”¨ä¾†ç©©å®šè¨“ç·´)ã€‚  
$`\alpha log \pi (a_{t+1}|s_{t+1})`$ : ä¸‹ä¸€æ­¥entropy bounsã€‚  
  
* ### Policy Loss (actorçš„Loss)
  
```math
L_\pi (\theta) = ğ”¼_{s_t \sim R,a_t \sim \pi_\theta} \left[\alpha log \pi_\theta (a_t|s_t) - Q_\omega (s_t,a_t) \right]
```
    
å®šç¾© :  
$`L_\pi (\theta)`$ : actor(policy network)è¦æœ€å°åŒ–çš„æå¤±ï¼Œ $`\theta`$ ç‚º policy network çš„åƒæ•¸ã€‚  
$`s_t \sim R`$ : ç‹€æ…‹å¾replay bufferæŠ½æ¨£(off-policy) ã€‚  
$`a_t \sim \pi_\theta(\cdot|s_t)`$ : åœ¨ state $`s_t`$ä¸‹ï¼Œå¾policy $`\pi`$ å–æ¨£å‹•ä½œã€‚  
$`\alpha log \pi_\theta (a_t|s_t)`$ : è¶Šç¢ºå®šçš„å‹•ä½œæ©Ÿç‡è¶Šæ¥è¿‘1 $`\to`$ log $`\pi`$ è¶Šå¤§(è² çš„)ï¼Œä¹Ÿå¯èªªæ˜¯é€™é …çš„æ•ˆæœç‚º **å¢åŠ entropyä¸”é¼“å‹µè¡Œç‚ºæ›´éš¨æ©Ÿ** ã€‚  
$`-Q_\omega (s_t,a_t)`$ : è‹¥Qå€¼è¶Šå¤§å‰‡é€™é …è² æ•¸å¤§ $`\to`$ æœ‰åŠ©æ–¼é™ä½lossï¼Œé¼“å‹µé¸æ“‡Qé«˜çš„è¡Œç‚ºã€‚  
  
* ###  $\alpha$  Loss
  
```math
L(\alpha) = ğ”¼_{a_t \sim \pi} \left[- \alpha log \pi (a_t|s_t) - \alpha H_0 \right]
```
**ç›®çš„ç‚ºè‡ªå‹•èª¿æ•´ $`\alpha`$ ä½¿ : $`ğ”¼[-log\pi] = H_0`$ï¼Œentropyè‡ªå‹•ç¶­æŒåœ¨å¸Œæœ›çš„æ°´æº–ã€‚**  
  
å®šç¾© :  
$`L(\alpha)`$ : å°ˆé–€ç”¨ä¾†æ›´æ–° $`\alpha`$ çš„ lossã€‚  
$`- \alpha log \pi (a_t|s_t)`$ : ç•¶ç­–ç•¥éæ–¼ç¢ºå®š(entropyå¤ªä½)æ™‚ï¼Œ $`log\pi`$æœƒè®Šå°ï¼Œlossåå¤§æœƒæ¨å‹• $`\alpha`$ æé«˜ $\to$ ä¿ƒä½¿ç­–ç•¥æ›´éš¨æ©Ÿã€‚  
$`-\alpha H_0`$ : $H_0$ æ˜¯ç›®æ¨™entropyï¼Œè®“ç­–ç•¥çš„entropyæœå›ºå®šç›®æ¨™é è¿‘ã€‚  
  
* ### Reparameterization Function(SAC core) 
  
```math
a_t = f_\theta(\epsilon_t ;s_t) ï¼Œ \epsilon_t \sim N(0,I)
```

**è®“policyæŠ½æ¨£è®Šæˆå¯å¾® $\to$ å¯ä»¥ç”¨backpropè¨“ç·´actorã€‚**  
  
å®šç¾© :  
$`f_\theta`$ : ä¸€å€‹å¯å¾®åˆ†å‡½æ•¸ï¼Œé€šå¸¸æ˜¯ $`f_\theta(\epsilon,s) = tanh(\mu_\theta(s_t) + \sigma _\theta(s_t) \cdot \epsilon_t)`$ ï¼ŒåŒ…å«é«˜æ–¯åˆ†å¸ƒå–æ¨£( $`u_t = \mu_\theta(s_t) + \sigma_\theta(s_t) \cdot \epsilon_t`$ ) è·Ÿ tanhç¸®æ”¾( $`a_t = tanh(u_t)`$ )  
$`\epsilon_t \sim N(0,I)`$ : å¾æ¨™æº–å¸¸æ…‹N(0,1) å–çš„noiseï¼Œæä¾›éš¨æ©Ÿæ€§ã€‚  
  
* ### Soft Value Function
  
```math
V(s_t) = ğ”¼_{a_t \sim \pi} \left[Q(s_t,a_t) - \alpha log \pi(a_t|s_t) \right]
```
  
**$`V(s_t)`$ = å¹³å‡ã€Œé¸åˆ°çš„Qå€¼ + è©²å‹•ä½œçš„æ¢ç´¢çå‹µã€ã€‚**  
å®šç¾© :  
$`V(s_t)`$ : åœ¨ç‹€æ…‹ $`s_t`$ çš„é æœŸç¸½åƒ¹å€¼ï¼Œä½†soft valueä¸åªæ˜¯rewardï¼Œä¹ŸåŒ…å« entropy bounsã€‚  
$`ğ”¼_{a_t \sim \pi}[\cdot]`$ : ç”±ç­–ç•¥ $`\pi`$ å–æ¨£å‹•ä½œã€‚  
$`Q(s_t,a_t)`$ : è©²å‹•ä½œçš„Q-value(å›å ±ç¸½æœŸæœ›)ã€‚  
$`-\alpha log \pi(a_t|s_t)`$ : ä»£è¡¨æ¢ç´¢bounsï¼Œè¶Šéš¨æ©Ÿè¶Šæœ‰çå‹µã€‚  

- ### æ‡‰ç”¨  
ç‹€æ…‹åƒ¹å€¼å‡½æ•¸ : è¡¡é‡ç•¶å‰å±€é¢çš„å¥½å£  
ç­–ç•¥å‡½æ•¸ : æ±ºå®šæ¨¡å‹åœ¨æ£‹ç›¤ä¸Šçš„èµ°æ³•é¸æ“‡å‚¾å‘  
loss function : LQâ€‹(Ï‰) --> è©•åƒ¹æ¯æ­¥æ£‹çš„å¥½å£  
                LÏ€â€‹(Î¸) --> è¼¸å‡ºæ¯ä¸€æ­¥æ£‹çš„æ©Ÿç‡åˆ†ä½ˆï¼Œè‹¥å¤šæ­¥æ£‹ Q å€¼æ¥è¿‘entropy æœƒé¼“å‹µæ¨¡å‹ç¹¼çºŒæ¢ç´¢å…¶ä»–å¯è¡Œèµ°æ³•  
                L(Î±)  --> æ¨¡å‹å¤ªä¿å®ˆã€è€æ˜¯èµ°åŒä¸€å¥—é–‹å±€å‰‡æé«˜ Î±ï¼Œå¼·è¿«å˜—è©¦æ–°ç­–ç•¥ ï¼›æ¨¡å‹å¤ªäº‚ã€åƒäº‚ä¸‹æ£‹å‰‡é™ä½ Î±ï¼Œä½¿æ±ºç­–æ›´ç©©å®š  
- ### Breakdown
![breakdown](https://github.com/user-attachments/assets/7e59e893-60b4-4168-ba47-7e828cf34e60)  
- ### API
- #### train_sac_chess_fullgame.py
<img width="733" height="165" alt="image" src="https://github.com/user-attachments/assets/9ebabe2d-ce20-4c76-a0de-b3333c11a329" />  
<img width="734" height="402" alt="image" src="https://github.com/user-attachments/assets/0c7ebe40-efa6-4748-bbd5-32bb60ba3f72" />    

num_steps : ç¸½äº’å‹•æ­¥æ•¸  
updates_per_step : åšå¹¾æ¬¡åƒæ•¸æ›´æ–°  
log_interval : å°å‡ºè¨“ç·´ç‹€æ³ï¼Œè¨ˆç®—å¹³å‡loss  
save_intervl : éš”å¤šå°‘global stepå­˜ä¸€æ¬¡
resume_prefix : æ‰‹å‹•æŒ‡å®š
anto_resume : è‡ªå‹•æ‰¾è¨“ç·´æœ€æ–°æª”
- #### env_chess_boss.py
<img width="405" height="168" alt="image" src="https://github.com/user-attachments/assets/38d623a1-05e5-42a4-97b0-661df3e63611" />  
<img width="346" height="168" alt="image" src="https://github.com/user-attachments/assets/d7495f7b-6ade-43ad-bf09-1c8aec3bcba1" />  
<img width="474" height="166" alt="image" src="https://github.com/user-attachments/assets/f762c2d0-eeb4-44ee-bc9d-ab9b549803c1" />  
<img width="733" height="254" alt="image" src="https://github.com/user-attachments/assets/68a35f9b-4b70-4ed1-871d-49e4d361c6ea" />  
<img width="472" height="167" alt="image" src="https://github.com/user-attachments/assets/87444f95-eb5d-4e32-a709-86a1868ce437" />  
<img width="474" height="225" alt="image" src="https://github.com/user-attachments/assets/c9ba5ae9-979e-4123-9897-a0b8338aaaac" />  
<img width="548" height="248" alt="image" src="https://github.com/user-attachments/assets/e4fc1fb7-0110-42f6-be1c-c461fe689be4" />  
<img width="547" height="250" alt="image" src="https://github.com/user-attachments/assets/d7eb97b7-4ccd-43f1-9786-50637678238a" />  
<img width="548" height="166" alt="image" src="https://github.com/user-attachments/assets/5be16d8e-4d80-4e21-af57-7a03b5d8eabf" />  
<img width="551" height="168" alt="image" src="https://github.com/user-attachments/assets/2c911992-a02a-440a-86c9-f6871864a02f" />  

  
- #### sac_agent.py
<img width="388" height="223" alt="image" src="https://github.com/user-attachments/assets/dbe3a1f9-c6db-4aef-9310-56c9fc1e2f18" />
<img width="389" height="163" alt="image" src="https://github.com/user-attachments/assets/89a1af84-9ae9-4bb9-a50b-3e28309e0210" />  
<img width="443" height="461" alt="image" src="https://github.com/user-attachments/assets/4e0b2853-5921-4366-ae2f-5fb679419f4d" />  
<img width="497" height="433" alt="image" src="https://github.com/user-attachments/assets/afeda22f-0f92-4d04-8cae-c436a7f43959" />  
<img width="496" height="166" alt="image" src="https://github.com/user-attachments/assets/ec34c4d7-4b99-4e66-a6f9-58d5bc9f18f3" />  
<img width="657" height="251" alt="image" src="https://github.com/user-attachments/assets/6bb07c7d-7ad9-487c-927d-5af665e964b7" />  
<img width="494" height="198" alt="image" src="https://github.com/user-attachments/assets/64b939fd-be81-48c4-8160-9fec174892d4" />    

"actor_loss": float,"critic1_loss": float,"critic2_loss": float,"alpha_loss": 0.0, "alpha_value": float(alpha),  
<img width="535" height="315" alt="image" src="https://github.com/user-attachments/assets/7dcb613e-d691-4689-bb0c-561b10bf359d" />  
<img width="542" height="227" alt="image" src="https://github.com/user-attachments/assets/5a1eb769-a24a-4a41-9bad-ea82896a0e52" />  
<img width="663" height="227" alt="image" src="https://github.com/user-attachments/assets/e4ef6a8b-a144-404f-8d11-ccb2bb98c60f" />  

- ### çµæœ
### step 0 ~ 1800000
**alpha = 0.05 ã€ tau = 0.005 ã€ 1r = 1e-4 ã€**   
**win = 1 ã€ lose = -1 ã€ draw = -0.4 ã€**  
**step_penalty = -0.0001 ã€ material_coeff = 0.02 ã€ check_bonus = 0.03 ã€ check_penalty = -0.03**  
### step 1600000 ~ 2600000
**alphaæ”¹æˆ0.03ã€tauæ”¹æˆ0.003ã€1ræ”¹æˆ5e-5**  
### step 2600000 ~ 3200000
**win = 1.2ã€lose = -1.0 ã€ draw = -0.6 ã€**  
**alpha = 0.02 ã€ actor_1r = 1e-4 ã€ critic_1r = 5e-5**  
  
<img width="640" height="480" alt="actor_loss_curve5" src="https://github.com/user-attachments/assets/d6a13021-8f5c-4283-a0bd-b2aca9d2ff7e" />  
  
<img width="640" height="480" alt="critic_loss_curve5" src="https://github.com/user-attachments/assets/bb9783a4-53dd-43e8-bf14-b85e59d0b4a2" />  
  
<img width="640" height="480" alt="eval_curve_fullgamev5" src="https://github.com/user-attachments/assets/8ad74c70-98d1-4171-9103-6f51ab4e175c" />  


- ### Reference
<https://hackmd.io/@shaoeChen/Bywb8YLKS/https%3A%2F%2Fhackmd.io%2F%40shaoeChen%2FSyez2AmFr#PPO-algorithm>PPOè§£é‡‹   
<https://medium.com/@kdk199604/ppo-efficient-stable-and-scalable-policy-optimization-15b5b9c74a88>PPOå¯¦éš›æ‡‰ç”¨æ¶æ§‹   
<https://hrl.boyuai.com/chapter/2/sac%E7%AE%97%E6%B3%95/>


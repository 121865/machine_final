Chess model  
===
ä»‹ç´¹  
---
ä½¿ç”¨Soft Actor-Critic (SAC)è¨“ç·´ç™½æ£‹ä¾†å°æŠ—Stockfish Bossï¼Œä¸¦ä»¥é—–é—œåˆ¶(Level 1~Level 3)ä¾†é€²è¡Œæˆæœå±•ç¤º  
* ç™½æ£‹ : RL Agent
* é»‘æ£‹ : Stockfish(ä¾ç…§Levelèª¿æ•´é›£åº¦)
* ç©æ³• : å¾Level 1é–‹å§‹ï¼ŒWinå‰‡å‡ç´šè‡³Level 2ï¼ŒLose / Drawä¸é™ç´šã€‚é€šé—œLevel 3å³çµæŸéŠæˆ²

æ¶æ§‹åœ–
---
<img width="1398" height="416" alt="image" src="https://github.com/user-attachments/assets/997473f9-7428-4680-8ecf-3170b677b273" />  

Breakdown
---
<img width="1184" height="892" alt="image" src="https://github.com/user-attachments/assets/1b183228-3a21-4f24-abd2-d8ababf4c10a" />  

API  
---
### env_chess_boss_sf.py  
| API                       | Input                                                               | Output                        | Method                                 |
| ------------------------- | ------------------------------------------------------------------- | ----------------------------- | ---------------------------------------|
| `ChessBossEnvSF.__init__` | `demo_mode: bool`<br>`soft_win_levelup: bool`<br>`engine_path: str` | `env instance`                | åˆå§‹åŒ–ç’°å¢ƒã€Stockfishã€é—œå¡è¦å‰‡          |
| `reset()`                 | â€“                                                                   | `state: np.ndarray`           | é‡ç½®æ£‹ç›¤ä¸¦å›å‚³åˆå§‹ç‹€æ…‹                   |
| `step(action)`            | `action: int (0~20479)`                                             | `(state, reward, done, info)` | åŸ·è¡Œä¸€å€‹ white actionï¼Œå…§å« black å›æ‡‰   |
| `legal_action_ids()`      | â€“                                                                   | `List[int]`                   | å›å‚³ç›®å‰åˆæ³• actionï¼ˆå·²ç·¨ç¢¼ï¼‰            |
| `set_global_step()`       | `step: int`                                                         | â€“                             | æä¾›è¨“ç·´é€²åº¦çµ¦ Bossï¼ˆcurriculum ç”¨ï¼‰     |  

### sac_agent.py  
| API                 | Input                                           | Output           | Method                             |
| ------------------- | ----------------------------------------------- | ---------------- | -----------------------------------|
| `SacAgent.__init__` | `state_shape`<br>`action_dim=20480`             | `agent instance` | åˆå§‹åŒ– SAC actor / critic           |
| `select_action()`   | `state`<br>`legal_actions`<br>`eval_mode: bool` | `action: int`    | é¸æ“‡åˆæ³• actionï¼ˆtrain / evalï¼‰     |
| `store()`           | `(s, a, r, s', done)`                           | â€“                | å„²å­˜ transition è‡³ replay buffer    |
| `update()`          | â€“                                               | `loss dict`      | åŸ·è¡Œ SAC æ›´æ–°                       |
| `save()`            | `path prefix`                                   | â€“                | å„²å­˜æ¨¡å‹                            |
| `load()`            | `path prefix`                                   | â€“                | è¼‰å…¥æ¨¡å‹                            |  
  
### train_sac_chess_fullgame_sf.py
| API                        | Input                             | Output           | Method                         |
| -------------------------- | --------------------------------- | ---------------- | -------------------------------|
| `run_training()`           | `env`<br>`agent`<br>`total_steps` | â€“                | ä¸»è¨“ç·´è¿´åœˆ                      |
| `sample_level()`           | â€“                                 | `level: int`     | ä¾ curriculum / è¨­å®šæ±ºå®šè¨“ç·´é—œå¡ |
| `is_new_game_event()`      | `info dict`                       | `bool`           | åˆ¤æ–·æ˜¯å¦ç™¼ç”Ÿ win/lose/draw      |
| `find_latest_checkpoint()` | `models_dir`                      | `prefix or None` | è‡ªå‹•æ¥çºŒè¨“ç·´                    |

### eval_curve_sf.py  
| API | Input | Output | Method |
| ----| ------| -------| -------|
| `list_checkpoints()`  | `models_dir: str` | `List[Tuple[int, str]]`<br>ï¼ˆ`[(step, prefix), ...]`ï¼‰  | æƒæ `models_dir` å…§çš„ `sac_chess_step*_actor.pth` ä¸¦è§£æ stepï¼Œå›å‚³æ’åºå¾Œçš„ checkpoint æ¸…å–® |
| `piece_value()` | `piece_type: int`ï¼ˆchess piece enumï¼‰| `int`  | å›å‚³å­åŠ›åˆ†æ•¸ï¼ˆP=1, N/B=3, R=5, Q=9, K=0ï¼‰|
| `material_diff_white()`  | `board: chess.Board`  | `float` | è¨ˆç®—ç›¤é¢å­åŠ›å·®ï¼šç™½æ–¹å­åŠ›ç¸½å’Œ âˆ’ é»‘æ–¹å­åŠ›ç¸½å’Œ  |
| `plies()` | `board: chess.Board` | `int` | ä¼°ç®—ç›®å‰åŠæ­¥æ•¸ï¼ˆply countï¼‰ |
| `filter_legal_actions()` | `legal_actions: List[int]`<br>`action_dim: int` | `List[int]`  | éæ¿¾è¶Šç•Œ actionï¼ˆä¸åœ¨ `[0, action_dim)`ï¼‰é¿å…è©•ä¼°æ™‚å´©æ½° |
| `eval_one_checkpoint()`| `env: ChessBossEnvSF`<br>`agent: SacAgent`<br>`games: int`<br>`eval_level: int`<br>`max_white_moves_per_game: int`<br>`action_dim: int`|`Dict[str, float]`<br>åŒ…å«ï¼š`win_rate/lose_rate/draw_rate`<br>`avg_terminal_material_diff`<br>`avg_terminal_plies`<br>`illegal_count` | è¼‰å…¥æŒ‡å®š checkpoint æ¬Šé‡å¾Œï¼Œåœ¨å›ºå®š Level è·‘å¤šç›¤è©•ä¼°ï¼Œçµ±è¨ˆå‹ç‡ã€å­åŠ›å·®ã€å°å±€é•·åº¦èˆ‡ illegal æ¬¡æ•¸ |
| `main()` | CLI argsï¼š<br>`--models_dir` `--engine_path` `--games` `--level` `--max_white_moves_per_game` `--save_prefix` `--action_dim` | ç”¢å‡º PNG æª” + matplotlib é¡¯ç¤º| æ‰¹æ¬¡è¿­ä»£æ‰€æœ‰ checkpointï¼Œå°å›ºå®š Level åšè©•ä¼°ä¸¦è¼¸å‡ºæ›²ç·šåœ–ï¼ˆW/L/Dã€Materialã€Pliesã€Illegalsï¼‰|

### plot_loss_curve.py
| API | Input  | Output | Method  |
| ----| ------ | ------ | ------- |
| `load_loss_csv()` | `path: str`ï¼ˆloss log CSV è·¯å¾‘ï¼‰     | `Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]`<br>ä¾åºç‚ºï¼š`steps, actor_losses, critic1_losses, critic2_losses, alpha_losses, alpha_values` | è®€å– CSVï¼šæ”¯æ´å…©ç¨®æ ¼å¼ï¼š<br>æ–°ç‰ˆ 6 æ¬„ï¼š`step,actor_loss,critic1_loss,critic2_loss,alpha_loss,alpha_value`<br>èˆŠç‰ˆ 5 æ¬„ï¼šç¼º `alpha_value` å‰‡å¡« `NaN`ï¼›å¿½ç•¥ç©ºè¡Œ/æ ¼å¼éŒ¯èª¤è¡Œ                                                                |
| `main()` | ç„¡ï¼ˆå›ºå®šå‘¼å« `load_loss_csv()` ä½¿ç”¨é è¨­è·¯å¾‘ï¼‰ | ç”¢å‡º PNG æª” + matplotlib è¦–çª—é¡¯ç¤º  | ç•«ä¸¦å­˜åœ–ï¼š<br>`critic_loss_curve_sf_v3.png`ï¼ˆcritic1/critic2ï¼‰<br>`actor_loss_curve_sf_v3.png`ï¼ˆactorï¼‰<br>`alpha_loss_curve_sf_v3.png`ï¼ˆalpha lossï¼‰<br>è‹¥ `alpha_values` ä¸æ˜¯å…¨ NaNï¼Œé¡å¤–å­˜ï¼š`alpha_value_curve_sf_v3.png` |

### demo_gui_sf.py
| API                       | Input                           | Output       | Method                         |
| ------------------------- | ------------------------------- | ------------ | -------------------------------|
| `ChessDemoGUI.__init__`   | `models_dir`<br>`step_delay_ms` | GUI instance | åˆå§‹åŒ– Demo èˆ‡æ¨¡å‹              |
| `start()`                 | â€“                               | â€“            | é–‹å§‹ / ç¹¼çºŒå±•ç¤º                 |
| `pause()`                 | â€“                               | â€“            | æš«åœ                           |
| `step_once()`             | â€“                               | â€“            | å–®æ­¥åŸ·è¡Œ                       |
| `_do_one_env_step()`      | â€“                               | â€“            | åŸ·è¡Œä¸€æ¬¡ env â†’ agent â†’ render  |
| `render_board_from_env()` | â€“                               | â€“            | ä¾ env.board ç•«æ£‹ç›¤            |
| `run()`                   | â€“                               | â€“            | å•Ÿå‹• GUI loop                  |


Loss Function  
---
### Critic Loss (Qçš„loss)  
æ”¾åœ¨sac_agent.pyçš„_update_once()ä¸­
```
q1 = critic1(states).gather(actions)
q2 = critic2(states).gather(actions)
```
```math
Q_1(s_t,a_t),Q_2(s_t,a_t)
```
```
target_q = r + (1-done) * gamma * V(next_state)
```
```math
y_t = r_t + (1-done_t) \gamma V(s_{t+1})
```
å…¶ä¸­doneè¡¨ç¤ºçµ‚æ­¢ç‹€æ…‹ï¼Œç”¨ä¾†å‘Šè¨´Criticé€™æ­¥æœ‰æ²’æœ‰æœªä¾†åƒ¹å€¼å¯ä»¥æœŸå¾…ã€‚  
è‹¥éçµ‚æ­¢ç‹€æ…‹(done = 0) : $y_t = r_t + \gamma V(s_{t+1})$  
ç‚ºçµ‚æ­¢ç‹€æ…‹(done = 1) : $y_t = r_t$

```
critic1_loss = mse(q1, target_q)
critic2_loss = mse(q2, target_q)
```

```math
L_{Q_i} = ğ”¼_{(s,a,r,s')}[(Q_i(s,a) - y_t)^2], i Ïµ {1,2}
```

### Actor Loss (Policyçš„objectiveï¼Œç”¨ä¾†æœ€å°åŒ–)  
æ”¾åœ¨sac_agent.pyçš„_update_once()ä¸­  
```
actor_loss = (probs * (self.alpha * log_probs - q_pi)).sum(dim=1).mean()
```
é€™æ˜¯ SAC çš„ policy objectiveï¼ˆå¸Œæœ›é«˜ Qã€åŒæ™‚ä¿ç•™Entropy)  
å› ç‚º optimizer æ˜¯åš minimizeï¼Œæ‰€ä»¥å¯«æˆ alpha*logÏ€ - Q çš„å½¢å¼ï¼ˆè¶Šå°è¶Šå¥½ï¼‰  
### Critic target  
æ”¾åœ¨sac_agent.pyçš„_update_once()ä¸­   

```
v_next = (next_probs * (q_next - self.alpha * next_log_probs)).sum(dim=1, keepdim=True)
target_q = rewards + (1.0 - dones) * self.gamma * v_next
```
  
```math
V(s') = \sum _a \pi(a|s')(min(Q_1(s',a),Q_2(s',a)) - \alpha log \pi (a|s'))
```
æ¨¡å‹æˆæœ
---
<img width="640" height="490" alt="è¢å¹•æ“·å–ç•«é¢ 2026-01-06 220740" src="https://github.com/user-attachments/assets/36ebf6c6-4bef-4720-a860-0be549dbac90" />
<img width="618" height="550" alt="è¢å¹•æ“·å–ç•«é¢ 2026-01-07 025227" src="https://github.com/user-attachments/assets/940aeff8-3e9d-43a9-9a5b-36efdb9f1da7" />
<img width="640" height="480" alt="actor_loss_curve_sf_v3" src="https://github.com/user-attachments/assets/165fe6e6-fae2-4e9f-8e88-dde6150358d3" />
<img width="640" height="480" alt="critic_loss_curve_sf_v3" src="https://github.com/user-attachments/assets/cefb07d4-4969-4f24-a2a7-32249be2edef" />

demoå½±ç‰‡ : [![SAC Chess Boss Demo](https://img.youtube.com/vi/7P4ckps1lrI/0.jpg)](https://youtu.be/7P4ckps1lrI)




